{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from keras.utils import to_categorical\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "x_data=iris.data\n",
    "y_data=iris.target\n",
    "\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "train_x, test_x, train_y, test_y = train_test_split(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(IrisDataset).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __getitem__(self, index):\n",
    "        return torch.from_numpy(np.array(self.x[index])), torch.from_numpy(np.array(self.y[index]))\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, targets):\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    _, pred = outputs.topk(1, 1, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(targets.view(1, -1))\n",
    "    n_correct_elems = correct.float()[0].sum().data\n",
    "\n",
    "    return n_correct_elems / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.l1 = torch.nn.Linear(4, 10)\n",
    "        self.l2 = torch.nn.Linear(10, 3)\n",
    "        self.sigmoid=torch.nn.Sigmoid()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.2, inplace=False)\n",
    "    def forward(self,x):\n",
    "        out1=self.relu(self.l1(x))\n",
    "        drop1=self.dropout(out1)\n",
    "        out2=self.l2(drop1)\n",
    "        y_pred = self.softmax(out2)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_dataset = IrisDataset(train_x, train_y)\n",
    "test_dataset = IrisDataset(test_x, test_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Model()\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "opt= torch.optim.SGD(model.parameters(),lr=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 [Train]\n",
      "Loss: 1.0429449081420898 Acc: 0.5966666539510092\n",
      "Epoch: 0 [Test]\n",
      "Loss: 1.0101237297058105 Acc: 0.7368420958518982\n",
      "Epoch: 1 [Train]\n",
      "Loss: 1.0379211107889812 Acc: 0.5600000222524008\n",
      "Epoch: 1 [Test]\n",
      "Loss: 0.998529851436615 Acc: 0.7368420958518982\n",
      "Epoch: 2 [Train]\n",
      "Loss: 1.0039017995198567 Acc: 0.5733333428700765\n",
      "Epoch: 2 [Test]\n",
      "Loss: 0.9888100624084473 Acc: 0.7368420958518982\n",
      "Epoch: 3 [Train]\n",
      "Loss: 1.0270059903462727 Acc: 0.5333333412806193\n",
      "Epoch: 3 [Test]\n",
      "Loss: 0.9821146726608276 Acc: 0.7894737124443054\n",
      "Epoch: 4 [Train]\n",
      "Loss: 1.0092992782592773 Acc: 0.5133333206176758\n",
      "Epoch: 4 [Test]\n",
      "Loss: 0.9741047024726868 Acc: 0.7631579041481018\n",
      "Epoch: 5 [Train]\n",
      "Loss: 0.9878815015157064 Acc: 0.601111094156901\n",
      "Epoch: 5 [Test]\n",
      "Loss: 0.9686558246612549 Acc: 0.7894737124443054\n",
      "Epoch: 6 [Train]\n",
      "Loss: 0.9831022421518961 Acc: 0.602222204208374\n",
      "Epoch: 6 [Test]\n",
      "Loss: 0.9592795372009277 Acc: 0.7631579041481018\n",
      "Epoch: 7 [Train]\n",
      "Loss: 0.9919508298238119 Acc: 0.5455555518468221\n",
      "Epoch: 7 [Test]\n",
      "Loss: 0.9542991518974304 Acc: 0.7631579041481018\n",
      "Epoch: 8 [Train]\n",
      "Loss: 0.99843962987264 Acc: 0.5377777814865112\n",
      "Epoch: 8 [Test]\n",
      "Loss: 0.9530114531517029 Acc: 0.9210526347160339\n",
      "Epoch: 9 [Train]\n",
      "Loss: 0.967719554901123 Acc: 0.7244444688161215\n",
      "Epoch: 9 [Test]\n",
      "Loss: 0.9463546276092529 Acc: 0.9210526347160339\n",
      "Epoch: 10 [Train]\n",
      "Loss: 0.9704778989156088 Acc: 0.5866666634877523\n",
      "Epoch: 10 [Test]\n",
      "Loss: 0.9389823079109192 Acc: 0.8421052694320679\n",
      "Epoch: 11 [Train]\n",
      "Loss: 0.9625413417816162 Acc: 0.6500000158945719\n",
      "Epoch: 11 [Test]\n",
      "Loss: 0.9259602427482605 Acc: 0.7368420958518982\n",
      "Epoch: 12 [Train]\n",
      "Loss: 0.972392717997233 Acc: 0.5944444338480631\n",
      "Epoch: 12 [Test]\n",
      "Loss: 0.9202490448951721 Acc: 0.7368420958518982\n",
      "Epoch: 13 [Train]\n",
      "Loss: 0.9462990760803223 Acc: 0.6355555852254232\n",
      "Epoch: 13 [Test]\n",
      "Loss: 0.9152811169624329 Acc: 0.7631579041481018\n",
      "Epoch: 14 [Train]\n",
      "Loss: 0.9707164764404297 Acc: 0.5444444417953491\n",
      "Epoch: 14 [Test]\n",
      "Loss: 0.9198558330535889 Acc: 0.8421052694320679\n",
      "Epoch: 15 [Train]\n",
      "Loss: 0.9510747591654459 Acc: 0.6555555661519369\n",
      "Epoch: 15 [Test]\n",
      "Loss: 0.9117811918258667 Acc: 0.7894737124443054\n",
      "Epoch: 16 [Train]\n",
      "Loss: 0.9715224901835123 Acc: 0.5588888724644979\n",
      "Epoch: 16 [Test]\n",
      "Loss: 0.914618968963623 Acc: 0.9210526347160339\n",
      "Epoch: 17 [Train]\n",
      "Loss: 0.944433848063151 Acc: 0.6955556074778239\n",
      "Epoch: 17 [Test]\n",
      "Loss: 0.9098140597343445 Acc: 0.9210526347160339\n",
      "Epoch: 18 [Train]\n",
      "Loss: 0.9270293712615967 Acc: 0.6877777576446533\n",
      "Epoch: 18 [Test]\n",
      "Loss: 0.9034001231193542 Acc: 0.9210526347160339\n",
      "Epoch: 19 [Train]\n",
      "Loss: 0.9482236703236898 Acc: 0.6344443957010905\n",
      "Epoch: 19 [Test]\n",
      "Loss: 0.9017030596733093 Acc: 0.9473684430122375\n",
      "Epoch: 20 [Train]\n",
      "Loss: 0.9173152446746826 Acc: 0.7444444497426351\n",
      "Epoch: 20 [Test]\n",
      "Loss: 0.8847892880439758 Acc: 0.7894737124443054\n",
      "Epoch: 21 [Train]\n",
      "Loss: 0.931149403254191 Acc: 0.6766666571299235\n",
      "Epoch: 21 [Test]\n",
      "Loss: 0.8818064332008362 Acc: 0.7894737124443054\n",
      "Epoch: 22 [Train]\n",
      "Loss: 0.938246488571167 Acc: 0.6411111354827881\n",
      "Epoch: 22 [Test]\n",
      "Loss: 0.8843976259231567 Acc: 0.9210526347160339\n",
      "Epoch: 23 [Train]\n",
      "Loss: 0.9139853318532308 Acc: 0.6755555470784506\n",
      "Epoch: 23 [Test]\n",
      "Loss: 0.8805035948753357 Acc: 0.9210526347160339\n",
      "Epoch: 24 [Train]\n",
      "Loss: 0.9341511726379395 Acc: 0.7300000190734863\n",
      "Epoch: 24 [Test]\n",
      "Loss: 0.876961350440979 Acc: 0.9210526347160339\n",
      "Epoch: 25 [Train]\n",
      "Loss: 0.8852814038594564 Acc: 0.7777777512868246\n",
      "Epoch: 25 [Test]\n",
      "Loss: 0.8660957217216492 Acc: 0.8157894611358643\n",
      "Epoch: 26 [Train]\n",
      "Loss: 0.9177457491556803 Acc: 0.6755555470784506\n",
      "Epoch: 26 [Test]\n",
      "Loss: 0.8649876117706299 Acc: 0.8684210777282715\n",
      "Epoch: 27 [Train]\n",
      "Loss: 0.9041380087534586 Acc: 0.688888947168986\n",
      "Epoch: 27 [Test]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizavetabatanina/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8670772910118103 Acc: 0.9210526347160339\n",
      "Epoch: 28 [Train]\n",
      "Loss: 0.8938786188761393 Acc: 0.7366666793823242\n",
      "Epoch: 28 [Test]\n",
      "Loss: 0.8593478202819824 Acc: 0.9210526347160339\n",
      "Epoch: 29 [Train]\n",
      "Loss: 0.9221169153849283 Acc: 0.6677777767181396\n",
      "Epoch: 29 [Test]\n",
      "Loss: 0.8742420077323914 Acc: 0.9210526347160339\n",
      "Epoch: 30 [Train]\n",
      "Loss: 0.8975969950358073 Acc: 0.7911110719045004\n",
      "Epoch: 30 [Test]\n",
      "Loss: 0.8579500913619995 Acc: 0.9210526347160339\n",
      "Epoch: 31 [Train]\n",
      "Loss: 0.9340558052062988 Acc: 0.7144443988800049\n",
      "Epoch: 31 [Test]\n",
      "Loss: 0.864666759967804 Acc: 0.9736841917037964\n",
      "Epoch: 32 [Train]\n",
      "Loss: 0.889604647954305 Acc: 0.7766666412353516\n",
      "Epoch: 32 [Test]\n",
      "Loss: 0.8490470051765442 Acc: 0.9210526347160339\n",
      "Epoch: 33 [Train]\n",
      "Loss: 0.9124702612559 Acc: 0.6944444179534912\n",
      "Epoch: 33 [Test]\n",
      "Loss: 0.8553348183631897 Acc: 0.9473684430122375\n",
      "Epoch: 34 [Train]\n",
      "Loss: 0.8903622627258301 Acc: 0.7699999809265137\n",
      "Epoch: 34 [Test]\n",
      "Loss: 0.8591601848602295 Acc: 0.9210526347160339\n",
      "Epoch: 35 [Train]\n",
      "Loss: 0.880168596903483 Acc: 0.7911110719045004\n",
      "Epoch: 35 [Test]\n",
      "Loss: 0.8443534970283508 Acc: 0.9473684430122375\n",
      "Epoch: 36 [Train]\n",
      "Loss: 0.8735802968343099 Acc: 0.7377777894337972\n",
      "Epoch: 36 [Test]\n",
      "Loss: 0.8398547172546387 Acc: 0.9473684430122375\n",
      "Epoch: 37 [Train]\n",
      "Loss: 0.8896216551462809 Acc: 0.7166666984558105\n",
      "Epoch: 37 [Test]\n",
      "Loss: 0.8372542858123779 Acc: 0.9473684430122375\n",
      "Epoch: 38 [Train]\n",
      "Loss: 0.8733967145284017 Acc: 0.7911110719045004\n",
      "Epoch: 38 [Test]\n",
      "Loss: 0.8299784660339355 Acc: 0.9210526347160339\n",
      "Epoch: 39 [Train]\n",
      "Loss: 0.8927437464396158 Acc: 0.7277777194976807\n",
      "Epoch: 39 [Test]\n",
      "Loss: 0.8314388394355774 Acc: 0.9473684430122375\n",
      "Epoch: 40 [Train]\n",
      "Loss: 0.8569205602010092 Acc: 0.8266666730244955\n",
      "Epoch: 40 [Test]\n",
      "Loss: 0.8286480903625488 Acc: 0.9473684430122375\n",
      "Epoch: 41 [Train]\n",
      "Loss: 0.8778136571248373 Acc: 0.7433333396911621\n",
      "Epoch: 41 [Test]\n",
      "Loss: 0.8316453099250793 Acc: 0.9736841917037964\n",
      "Epoch: 42 [Train]\n",
      "Loss: 0.8705964883168539 Acc: 0.707777738571167\n",
      "Epoch: 42 [Test]\n",
      "Loss: 0.8410649299621582 Acc: 0.8947368264198303\n",
      "Epoch: 43 [Train]\n",
      "Loss: 0.8443052768707275 Acc: 0.8466666539510092\n",
      "Epoch: 43 [Test]\n",
      "Loss: 0.8297269344329834 Acc: 0.9473684430122375\n",
      "Epoch: 44 [Train]\n",
      "Loss: 0.8578395843505859 Acc: 0.851111094156901\n",
      "Epoch: 44 [Test]\n",
      "Loss: 0.8234536647796631 Acc: 0.9736841917037964\n",
      "Epoch: 45 [Train]\n",
      "Loss: 0.8598864078521729 Acc: 0.8666666348775228\n",
      "Epoch: 45 [Test]\n",
      "Loss: 0.8306770920753479 Acc: 0.8947368264198303\n",
      "Epoch: 46 [Train]\n",
      "Loss: 0.8469409942626953 Acc: 0.8333333333333334\n",
      "Epoch: 46 [Test]\n",
      "Loss: 0.8245256543159485 Acc: 0.9210526347160339\n",
      "Epoch: 47 [Train]\n",
      "Loss: 0.8968871434529623 Acc: 0.6866666475931803\n",
      "Epoch: 47 [Test]\n",
      "Loss: 0.8120014667510986 Acc: 0.9473684430122375\n",
      "Epoch: 48 [Train]\n",
      "Loss: 0.8841050465901693 Acc: 0.6999999682108561\n",
      "Epoch: 48 [Test]\n",
      "Loss: 0.8168106079101562 Acc: 1.0\n",
      "Epoch: 49 [Train]\n",
      "Loss: 0.8399108250935873 Acc: 0.8244444529215494\n",
      "Epoch: 49 [Test]\n",
      "Loss: 0.807598888874054 Acc: 0.9473684430122375\n",
      "Epoch: 50 [Train]\n",
      "Loss: 0.8639678160349528 Acc: 0.7833333015441895\n",
      "Epoch: 50 [Test]\n",
      "Loss: 0.815549373626709 Acc: 0.9473684430122375\n",
      "Epoch: 51 [Train]\n",
      "Loss: 0.8485154310862223 Acc: 0.8588888645172119\n",
      "Epoch: 51 [Test]\n",
      "Loss: 0.7943787574768066 Acc: 0.9210526347160339\n",
      "Epoch: 52 [Train]\n",
      "Loss: 0.8542993068695068 Acc: 0.7977777322133383\n",
      "Epoch: 52 [Test]\n",
      "Loss: 0.8010088801383972 Acc: 0.9473684430122375\n",
      "Epoch: 53 [Train]\n",
      "Loss: 0.820451021194458 Acc: 0.7988889217376709\n",
      "Epoch: 53 [Test]\n",
      "Loss: 0.7915833592414856 Acc: 0.9473684430122375\n",
      "Epoch: 54 [Train]\n",
      "Loss: 0.8386914730072021 Acc: 0.811111052831014\n",
      "Epoch: 54 [Test]\n",
      "Loss: 0.7887529730796814 Acc: 0.9210526347160339\n",
      "Epoch: 55 [Train]\n",
      "Loss: 0.8493684132893881 Acc: 0.7833333015441895\n",
      "Epoch: 55 [Test]\n",
      "Loss: 0.7933599352836609 Acc: 0.9473684430122375\n",
      "Epoch: 56 [Train]\n",
      "Loss: 0.8365582625071207 Acc: 0.8255555629730225\n",
      "Epoch: 56 [Test]\n",
      "Loss: 0.7911189794540405 Acc: 0.9473684430122375\n",
      "Epoch: 57 [Train]\n",
      "Loss: 0.8323798179626465 Acc: 0.8599999745686849\n",
      "Epoch: 57 [Test]\n",
      "Loss: 0.7813054919242859 Acc: 0.9210526347160339\n",
      "Epoch: 58 [Train]\n",
      "Loss: 0.845032533009847 Acc: 0.751111110051473\n",
      "Epoch: 58 [Test]\n",
      "Loss: 0.7894189953804016 Acc: 0.9736841917037964\n",
      "Epoch: 59 [Train]\n",
      "Loss: 0.8365968068440756 Acc: 0.8322222232818604\n",
      "Epoch: 59 [Test]\n",
      "Loss: 0.7842071652412415 Acc: 0.9473684430122375\n",
      "Epoch: 60 [Train]\n",
      "Loss: 0.8151933352152506 Acc: 0.8733332951863607\n",
      "Epoch: 60 [Test]\n",
      "Loss: 0.7870533466339111 Acc: 0.9736841917037964\n",
      "Epoch: 61 [Train]\n",
      "Loss: 0.8290208180745443 Acc: 0.8177777926127116\n",
      "Epoch: 61 [Test]\n",
      "Loss: 0.775148868560791 Acc: 0.9210526347160339\n",
      "Epoch: 62 [Train]\n",
      "Loss: 0.8115346431732178 Acc: 0.8266666730244955\n",
      "Epoch: 62 [Test]\n",
      "Loss: 0.7744242548942566 Acc: 0.9473684430122375\n",
      "Epoch: 63 [Train]\n",
      "Loss: 0.8262232144673666 Acc: 0.7911110719045004\n",
      "Epoch: 63 [Test]\n",
      "Loss: 0.7824336290359497 Acc: 0.9736841917037964\n",
      "Epoch: 64 [Train]\n",
      "Loss: 0.8143674532572428 Acc: 0.8455555438995361\n",
      "Epoch: 64 [Test]\n",
      "Loss: 0.7877573370933533 Acc: 0.9736841917037964\n",
      "Epoch: 65 [Train]\n",
      "Loss: 0.8211971918741862 Acc: 0.8177777926127116\n",
      "Epoch: 65 [Test]\n",
      "Loss: 0.7677614092826843 Acc: 0.9210526347160339\n",
      "Epoch: 66 [Train]\n",
      "Loss: 0.7977891763051351 Acc: 0.8666666348775228\n",
      "Epoch: 66 [Test]\n",
      "Loss: 0.7730214595794678 Acc: 0.9473684430122375\n",
      "Epoch: 67 [Train]\n",
      "Loss: 0.8031288782755533 Acc: 0.8311111132303873\n",
      "Epoch: 67 [Test]\n",
      "Loss: 0.784781277179718 Acc: 0.9736841917037964\n",
      "Epoch: 68 [Train]\n",
      "Loss: 0.8313306172688802 Acc: 0.754444440205892\n",
      "Epoch: 68 [Test]\n",
      "Loss: 0.7661690711975098 Acc: 0.9473684430122375\n",
      "Epoch: 69 [Train]\n",
      "Loss: 0.8191448847452799 Acc: 0.7644444306691488\n",
      "Epoch: 69 [Test]\n",
      "Loss: 0.7633503079414368 Acc: 0.9473684430122375\n",
      "Epoch: 70 [Train]\n",
      "Loss: 0.8211042086283366 Acc: 0.7966666221618652\n",
      "Epoch: 70 [Test]\n",
      "Loss: 0.764545738697052 Acc: 0.9473684430122375\n",
      "Epoch: 71 [Train]\n",
      "Loss: 0.8195293744405111 Acc: 0.8255555629730225\n",
      "Epoch: 71 [Test]\n",
      "Loss: 0.759212851524353 Acc: 0.9473684430122375\n",
      "Epoch: 72 [Train]\n",
      "Loss: 0.8487013975779215 Acc: 0.7444444497426351\n",
      "Epoch: 72 [Test]\n",
      "Loss: 0.7605347633361816 Acc: 0.9473684430122375\n",
      "Epoch: 73 [Train]\n",
      "Loss: 0.8461421330769857 Acc: 0.8188889026641846\n",
      "Epoch: 73 [Test]\n",
      "Loss: 0.7625115513801575 Acc: 0.9473684430122375\n",
      "Epoch: 74 [Train]\n",
      "Loss: 0.8520377477010092 Acc: 0.7755556106567383\n",
      "Epoch: 74 [Test]\n",
      "Loss: 0.7866183519363403 Acc: 0.8947368264198303\n",
      "Epoch: 75 [Train]\n",
      "Loss: 0.8431743780771891 Acc: 0.7566666603088379\n",
      "Epoch: 75 [Test]\n",
      "Loss: 0.7823413610458374 Acc: 0.9210526347160339\n",
      "Epoch: 76 [Train]\n",
      "Loss: 0.815649668375651 Acc: 0.8655555248260498\n",
      "Epoch: 76 [Test]\n",
      "Loss: 0.7714477777481079 Acc: 0.9736841917037964\n",
      "Epoch: 77 [Train]\n",
      "Loss: 0.8005690574645996 Acc: 0.8322222232818604\n",
      "Epoch: 77 [Test]\n",
      "Loss: 0.7738330364227295 Acc: 0.9210526347160339\n",
      "Epoch: 78 [Train]\n",
      "Loss: 0.838476816813151 Acc: 0.8022222518920898\n",
      "Epoch: 78 [Test]\n",
      "Loss: 0.7581807971000671 Acc: 0.9473684430122375\n",
      "Epoch: 79 [Train]\n",
      "Loss: 0.8115693728129069 Acc: 0.8111111323038737\n",
      "Epoch: 79 [Test]\n",
      "Loss: 0.7559332251548767 Acc: 0.9473684430122375\n",
      "Epoch: 80 [Train]\n",
      "Loss: 0.7997066179911295 Acc: 0.8644444147745768\n",
      "Epoch: 80 [Test]\n",
      "Loss: 0.7574419379234314 Acc: 0.9736841917037964\n",
      "Epoch: 81 [Train]\n",
      "Loss: 0.7956650257110596 Acc: 0.852222204208374\n",
      "Epoch: 81 [Test]\n",
      "Loss: 0.7640154361724854 Acc: 0.9736841917037964\n",
      "Epoch: 82 [Train]\n",
      "Loss: 0.7943010330200195 Acc: 0.8922222455342611\n",
      "Epoch: 82 [Test]\n",
      "Loss: 0.7594103813171387 Acc: 1.0\n",
      "Epoch: 83 [Train]\n",
      "Loss: 0.7990268071492513 Acc: 0.8722222646077474\n",
      "Epoch: 83 [Test]\n",
      "Loss: 0.7495603561401367 Acc: 0.9736841917037964\n",
      "Epoch: 84 [Train]\n",
      "Loss: 0.8271659215291342 Acc: 0.7688888708750407\n",
      "Epoch: 84 [Test]\n",
      "Loss: 0.7543199062347412 Acc: 1.0\n",
      "Epoch: 85 [Train]\n",
      "Loss: 0.7953266302744547 Acc: 0.8644444147745768\n",
      "Epoch: 85 [Test]\n",
      "Loss: 0.7436381578445435 Acc: 0.9473684430122375\n",
      "Epoch: 86 [Train]\n",
      "Loss: 0.8066204388936361 Acc: 0.7844444115956625\n",
      "Epoch: 86 [Test]\n",
      "Loss: 0.743380606174469 Acc: 0.9473684430122375\n",
      "Epoch: 87 [Train]\n",
      "Loss: 0.8110442956288656 Acc: 0.7977777322133383\n",
      "Epoch: 87 [Test]\n",
      "Loss: 0.746369481086731 Acc: 0.9736841917037964\n",
      "Epoch: 88 [Train]\n",
      "Loss: 0.7783055305480957 Acc: 0.8588888645172119\n",
      "Epoch: 88 [Test]\n",
      "Loss: 0.7500212788581848 Acc: 1.0\n",
      "Epoch: 89 [Train]\n",
      "Loss: 0.7815756797790527 Acc: 0.8866666158040365\n",
      "Epoch: 89 [Test]\n",
      "Loss: 0.7462043166160583 Acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "dfor epoch in range(90):\n",
    "    for step in ['train', 'test']:\n",
    "        print(f'Epoch: {epoch} [{step.capitalize()}]')\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        if step == 'train':\n",
    "            loader = train_loader\n",
    "            model.train()\n",
    "        else:\n",
    "            loader = test_loader\n",
    "            model.eval()\n",
    "        for i, (data, target) in enumerate(loader):\n",
    "            inputs = Variable(data)\n",
    "            outputs = Variable(target)\n",
    "            y_pred_val=model(inputs)\n",
    "            acc = calculate_accuracy(y_pred_val, outputs)\n",
    "            accuracies.append(acc)\n",
    "            loss=criterion(y_pred_val, outputs)\n",
    "            losses.append(loss.data)\n",
    "            if step == 'train':\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "        print(f'Loss: {np.sum(losses)/len(loader)}', f'Acc: {np.sum(accuracies)/len(loader)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatedAnnealing:\n",
    "    def __init__(self, T, max_t, min_T, energy_xt, model):\n",
    "        self.T = T\n",
    "        self.t = 0\n",
    "        self.params_shapes = [p.shape for p in model.parameters()]\n",
    "        self.shape_x = sum([np.prod(sh) for sh in self.params_shapes])\n",
    "        self.max_t = max_t\n",
    "        self.min_T = min_T\n",
    "        x0 = self.extract_parameters(model)\n",
    "        self.x_t = x0\n",
    "        self.x_prime = x0\n",
    "        self.energy_xt =  energy_xt\n",
    "        self.alpha = 0.99 # exponential?\n",
    "        self.best_position = (energy_xt, x0)\n",
    "    def accept(self, energy_x):\n",
    "        alpha = np.random.uniform(0, 1)\n",
    "        if self.__prob(self.energy_xt) == 0.0:\n",
    "            return True\n",
    "        ratio = self.__prob(energy_x)/ self.__prob(self.energy_xt)\n",
    "        if ratio >= alpha:\n",
    "            return True\n",
    "        return False\n",
    "    def step(self, model, energy_x):\n",
    "        if self.accept(energy_x):\n",
    "            self.x_t = self.x_prime\n",
    "            self.t += 1\n",
    "            if energy_x < self.best_position[0]:\n",
    "                self.best_position = (energy_x, self.x_prime)\n",
    "            if self.t > self.max_t:\n",
    "                self.T *= self.alpha\n",
    "                self.t = 0\n",
    "        self.generate() # new x_prime\n",
    "        self.set_parameters(model, self.x_prime) # set new x_prime_params\n",
    "        \n",
    "    def extract_parameters(self, model):\n",
    "        array = np.concatenate([p.data.flatten() for p in model.parameters()])\n",
    "        return array\n",
    "    def set_parameters(self, model, x):\n",
    "        shapes = [p.shape for p in model.parameters()]\n",
    "        lengths = [np.prod(sh) for sh in shapes]\n",
    "        cuts_ids = [lengths[0]]\n",
    "        for i in range(1, len(lengths)):\n",
    "            cuts_ids.append(cuts_ids[i-1] + lengths[i])\n",
    "        cuts = np.array_split(x, cuts_ids)\n",
    "        new_params = []\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            param.data = torch.tensor(np.reshape(cuts[i], shapes[i]), dtype=torch.float32)\n",
    "        \n",
    "    def __prob(self, energy_x):\n",
    "        return (np.e)**(-energy_x/self.T)\n",
    "    \n",
    "    def generate(self):\n",
    "        self.x_prime = np.random.normal(loc=np.mean(self.x_t), size=self.shape_x).astype(np.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizavetabatanina/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "model =Model()\n",
    "y_pred_val=model(Variable(torch.Tensor(train_x)))\n",
    "loss=criterion(y_pred_val, torch.from_numpy(train_y))\n",
    "sa_opt = SimulatedAnnealing(float(loss.data)*2, 60, 0.01, float(loss.data), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_dataset = IrisDataset(train_x, train_y)\n",
    "test_dataset = IrisDataset(test_x, test_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizavetabatanina/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 [Train]\n",
      "T:2.2771453857421875, Best: 1.1382832527160645, t: 3\n",
      "Loss: 1.1689612865447998 Acc: 0.32555556297302246\n",
      "================================================================================\n",
      "Epoch: 0 [Test]\n",
      "T:2.2771453857421875, Best: 1.1382832527160645, t: 3\n",
      "Loss: 1.0835131406784058 Acc: 0.42105263471603394\n",
      "================================================================================\n",
      "Epoch: 600 [Train]\n",
      "T:1.7186055423863842, Best: 0.8344288468360901, t: 42\n",
      "Loss: 1.0693312486012776 Acc: 0.4144444465637207\n",
      "================================================================================\n",
      "Epoch: 600 [Test]\n",
      "T:1.7186055423863842, Best: 0.8344288468360901, t: 42\n",
      "Loss: 1.1073907613754272 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 1200 [Train]\n",
      "T:1.2840941024347232, Best: 0.8344288468360901, t: 54\n",
      "Loss: 1.1597588857014973 Acc: 0.25555557012557983\n",
      "================================================================================\n",
      "Epoch: 1200 [Test]\n",
      "T:1.2840941024347232, Best: 0.8344288468360901, t: 54\n",
      "Loss: 1.1665098667144775 Acc: 0.2631579041481018\n",
      "================================================================================\n",
      "Epoch: 1800 [Train]\n",
      "T:0.9594392798350035, Best: 0.8344288468360901, t: 3\n",
      "Loss: 1.2008606592814128 Acc: 0.352222204208374\n",
      "================================================================================\n",
      "Epoch: 1800 [Test]\n",
      "T:0.9594392798350035, Best: 0.8344288468360901, t: 3\n",
      "Loss: 1.0468164682388306 Acc: 0.5\n",
      "================================================================================\n",
      "Epoch: 2400 [Train]\n",
      "T:0.7388096412531788, Best: 0.8014445900917053, t: 41\n",
      "Loss: 1.2917606035868328 Acc: 0.2633333404858907\n",
      "================================================================================\n",
      "Epoch: 2400 [Test]\n",
      "T:0.7388096412531788, Best: 0.8014445900917053, t: 41\n",
      "Loss: 1.1830238103866577 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 2499 [Train]\n",
      "T:0.709697593537335, Best: 0.8014445900917053, t: 56\n",
      "Loss: 1.1821705500284831 Acc: 0.3677777449289958\n",
      "================================================================================\n",
      "Epoch: 2499 [Test]\n",
      "T:0.709697593537335, Best: 0.8014445900917053, t: 56\n",
      "Loss: 1.1830238103866577 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 3000 [Train]\n",
      "T:0.5689153003017758, Best: 0.8014445900917053, t: 11\n",
      "Loss: 1.1472524007161458 Acc: 0.4011111259460449\n",
      "================================================================================\n",
      "Epoch: 3000 [Test]\n",
      "T:0.5689153003017758, Best: 0.8014445900917053, t: 11\n",
      "Loss: 1.1830233335494995 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 3600 [Train]\n",
      "T:0.42937134992299686, Best: 0.8014445900917053, t: 37\n",
      "Loss: 1.134135643641154 Acc: 0.2966666618982951\n",
      "================================================================================\n",
      "Epoch: 3600 [Test]\n",
      "T:0.42937134992299686, Best: 0.8014445900917053, t: 37\n",
      "Loss: 1.123162031173706 Acc: 0.2631579041481018\n",
      "================================================================================\n",
      "Epoch: 4200 [Train]\n",
      "T:0.32081429076795015, Best: 0.8014445900917053, t: 26\n",
      "Loss: 1.1318991978963215 Acc: 0.3111111323038737\n",
      "================================================================================\n",
      "Epoch: 4200 [Test]\n",
      "T:0.32081429076795015, Best: 0.8014445900917053, t: 26\n",
      "Loss: 1.1474559307098389 Acc: 0.2631579041481018\n",
      "================================================================================\n",
      "Epoch: 4800 [Train]\n",
      "T:0.24212473285312625, Best: 0.8014445900917053, t: 57\n",
      "Loss: 1.1692622502644856 Acc: 0.2833333412806193\n",
      "================================================================================\n",
      "Epoch: 4800 [Test]\n",
      "T:0.24212473285312625, Best: 0.8014445900917053, t: 57\n",
      "Loss: 1.1157957315444946 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 5400 [Train]\n",
      "T:0.18458202605381951, Best: 0.7301239967346191, t: 31\n",
      "Loss: 1.2313933372497559 Acc: 0.30222223202387494\n",
      "================================================================================\n",
      "Epoch: 5400 [Test]\n",
      "T:0.18458202605381951, Best: 0.7301239967346191, t: 31\n",
      "Loss: 1.1903502941131592 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 6000 [Train]\n",
      "T:0.1435718424258306, Best: 0.7301239967346191, t: 12\n",
      "Loss: 1.1292913754781086 Acc: 0.303333322207133\n",
      "================================================================================\n",
      "Epoch: 6000 [Test]\n",
      "T:0.1435718424258306, Best: 0.7301239967346191, t: 12\n",
      "Loss: 1.160025715827942 Acc: 0.2631579041481018\n",
      "================================================================================\n",
      "Epoch: 6600 [Train]\n",
      "T:0.10945094805872234, Best: 0.7301239967346191, t: 48\n",
      "Loss: 1.122963269551595 Acc: 0.3377777735392253\n",
      "================================================================================\n",
      "Epoch: 6600 [Test]\n",
      "T:0.10945094805872234, Best: 0.7301239967346191, t: 48\n",
      "Loss: 1.1203962564468384 Acc: 0.2631579041481018\n",
      "================================================================================\n",
      "Epoch: 7200 [Train]\n",
      "T:0.08343913283095021, Best: 0.7301239967346191, t: 44\n",
      "Loss: 1.1423474152882893 Acc: 0.3100000023841858\n",
      "================================================================================\n",
      "Epoch: 7200 [Test]\n",
      "T:0.08343913283095021, Best: 0.7301239967346191, t: 44\n",
      "Loss: 1.0907305479049683 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 7800 [Train]\n",
      "T:0.06360921500511511, Best: 0.7301239967346191, t: 21\n",
      "Loss: 1.1843563715616863 Acc: 0.20666666825612387\n",
      "================================================================================\n",
      "Epoch: 7800 [Test]\n",
      "T:0.06360921500511511, Best: 0.7301239967346191, t: 21\n",
      "Loss: 1.0896341800689697 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 8400 [Train]\n",
      "T:0.04898184002473532, Best: 0.7301239967346191, t: 29\n",
      "Loss: 1.0814865430196126 Acc: 0.3988888661066691\n",
      "================================================================================\n",
      "Epoch: 8400 [Test]\n",
      "T:0.04898184002473532, Best: 0.7301239967346191, t: 29\n",
      "Loss: 1.1273335218429565 Acc: 0.2631579041481018\n",
      "================================================================================\n",
      "Epoch: 9000 [Train]\n",
      "T:0.038099121393911146, Best: 0.7301239967346191, t: 49\n",
      "Loss: 1.1365056037902832 Acc: 0.2900000015894572\n",
      "================================================================================\n",
      "Epoch: 9000 [Test]\n",
      "T:0.038099121393911146, Best: 0.7301239967346191, t: 49\n",
      "Loss: 1.0914217233657837 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 9600 [Train]\n",
      "T:0.02963431039452507, Best: 0.7301239967346191, t: 44\n",
      "Loss: 1.1138227780659993 Acc: 0.30444443225860596\n",
      "================================================================================\n",
      "Epoch: 9600 [Test]\n",
      "T:0.02963431039452507, Best: 0.7301239967346191, t: 44\n",
      "Loss: 1.1080029010772705 Acc: 0.2631579041481018\n",
      "================================================================================\n",
      "Epoch: 10200 [Train]\n",
      "T:0.023283029894874498, Best: 0.7301239967346191, t: 54\n",
      "Loss: 1.1490976015726726 Acc: 0.2611111005147298\n",
      "================================================================================\n",
      "Epoch: 10200 [Test]\n",
      "T:0.023283029894874498, Best: 0.7301239967346191, t: 54\n",
      "Loss: 1.1355266571044922 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 10800 [Train]\n",
      "T:0.018110037963762253, Best: 0.7301239967346191, t: 13\n",
      "Loss: 1.1121652921040852 Acc: 0.3288888931274414\n",
      "================================================================================\n",
      "Epoch: 10800 [Test]\n",
      "T:0.018110037963762253, Best: 0.7301239967346191, t: 13\n",
      "Loss: 1.090567946434021 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 11400 [Train]\n",
      "T:0.014517560409465235, Best: 0.7301239967346191, t: 30\n",
      "Loss: 1.1471896171569824 Acc: 0.25555557012557983\n",
      "================================================================================\n",
      "Epoch: 11400 [Test]\n",
      "T:0.014517560409465235, Best: 0.7301239967346191, t: 30\n",
      "Loss: 1.1063963174819946 Acc: 0.3684210479259491\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12000 [Train]\n",
      "T:0.011637720509708328, Best: 0.7301239967346191, t: 57\n",
      "Loss: 1.110044797261556 Acc: 0.38555554548899335\n",
      "================================================================================\n",
      "Epoch: 12000 [Test]\n",
      "T:0.011637720509708328, Best: 0.7301239967346191, t: 57\n",
      "Loss: 1.1108384132385254 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 12600 [Train]\n",
      "T:0.009518572342708021, Best: 0.7301239967346191, t: 20\n",
      "Loss: 1.110204855600993 Acc: 0.34555554389953613\n",
      "================================================================================\n",
      "Epoch: 12600 [Test]\n",
      "T:0.009518572342708021, Best: 0.7301239967346191, t: 20\n",
      "Loss: 1.1138323545455933 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 13200 [Train]\n",
      "T:0.007554074971068538, Best: 0.7301239967346191, t: 6\n",
      "Loss: 1.127679745356242 Acc: 0.34555554389953613\n",
      "================================================================================\n",
      "Epoch: 13200 [Test]\n",
      "T:0.007554074971068538, Best: 0.7301239967346191, t: 6\n",
      "Loss: 1.1037757396697998 Acc: 0.2631579041481018\n",
      "================================================================================\n",
      "Epoch: 13800 [Train]\n",
      "T:0.006055577572479822, Best: 0.7301239967346191, t: 0\n",
      "Loss: 1.0984560648600261 Acc: 0.36666667461395264\n",
      "================================================================================\n",
      "Epoch: 13800 [Test]\n",
      "T:0.006055577572479822, Best: 0.7301239967346191, t: 0\n",
      "Loss: 1.151438593864441 Acc: 0.2631579041481018\n",
      "================================================================================\n",
      "Epoch: 14400 [Train]\n",
      "T:0.004854336219426424, Best: 0.7301239967346191, t: 1\n",
      "Loss: 1.1232913335164387 Acc: 0.3233333428700765\n",
      "================================================================================\n",
      "Epoch: 14400 [Test]\n",
      "T:0.004854336219426424, Best: 0.7301239967346191, t: 1\n",
      "Loss: 1.116689920425415 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 15000 [Train]\n",
      "T:0.0039306913185854, Best: 0.7301239967346191, t: 30\n",
      "Loss: 1.167038122812907 Acc: 0.2688888907432556\n",
      "================================================================================\n",
      "Epoch: 15000 [Test]\n",
      "T:0.0039306913185854, Best: 0.7301239967346191, t: 30\n",
      "Loss: 1.1243897676467896 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 15600 [Train]\n",
      "T:0.0031509623990136244, Best: 0.7301239967346191, t: 9\n",
      "Loss: 1.1035997072855632 Acc: 0.39222220579783124\n",
      "================================================================================\n",
      "Epoch: 15600 [Test]\n",
      "T:0.0031509623990136244, Best: 0.7301239967346191, t: 9\n",
      "Loss: 1.1219229698181152 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 16200 [Train]\n",
      "T:0.0025514220661986436, Best: 0.7301239967346191, t: 34\n",
      "Loss: 1.1236170132954915 Acc: 0.3655555645624797\n",
      "================================================================================\n",
      "Epoch: 16200 [Test]\n",
      "T:0.0025514220661986436, Best: 0.7301239967346191, t: 34\n",
      "Loss: 1.0936819314956665 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 16800 [Train]\n",
      "T:0.0020659575505957064, Best: 0.7301239967346191, t: 43\n",
      "Loss: 1.1570191383361816 Acc: 0.2833333412806193\n",
      "================================================================================\n",
      "Epoch: 16800 [Test]\n",
      "T:0.0020659575505957064, Best: 0.7301239967346191, t: 43\n",
      "Loss: 1.0949041843414307 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 17400 [Train]\n",
      "T:0.0016561347692466792, Best: 0.7301239967346191, t: 10\n",
      "Loss: 1.181765079498291 Acc: 0.2755555510520935\n",
      "================================================================================\n",
      "Epoch: 17400 [Test]\n",
      "T:0.0016561347692466792, Best: 0.7301239967346191, t: 10\n",
      "Loss: 1.1121622323989868 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 18000 [Train]\n",
      "T:0.001288176997563644, Best: 0.7301239967346191, t: 50\n",
      "Loss: 1.1380321184794109 Acc: 0.2688888907432556\n",
      "================================================================================\n",
      "Epoch: 18000 [Test]\n",
      "T:0.001288176997563644, Best: 0.7301239967346191, t: 50\n",
      "Loss: 1.1006474494934082 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 18600 [Train]\n",
      "T:0.0009528650060880217, Best: 0.7301239967346191, t: 20\n",
      "Loss: 1.1421728134155273 Acc: 0.30444443225860596\n",
      "================================================================================\n",
      "Epoch: 18600 [Test]\n",
      "T:0.0009528650060880217, Best: 0.7301239967346191, t: 20\n",
      "Loss: 1.1249430179595947 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 19200 [Train]\n",
      "T:0.0007119541422140763, Best: 0.7301239967346191, t: 51\n",
      "Loss: 1.1488800048828125 Acc: 0.36444445451100665\n",
      "================================================================================\n",
      "Epoch: 19200 [Test]\n",
      "T:0.0007119541422140763, Best: 0.7301239967346191, t: 51\n",
      "Loss: 1.1686062812805176 Acc: 0.3684210479259491\n",
      "================================================================================\n",
      "Epoch: 19800 [Train]\n",
      "T:0.000526632744831085, Best: 0.7301239967346191, t: 21\n",
      "Loss: 1.1714385350545247 Acc: 0.2966666618982951\n",
      "================================================================================\n",
      "Epoch: 19800 [Test]\n",
      "T:0.000526632744831085, Best: 0.7301239967346191, t: 21\n",
      "Loss: 1.1076667308807373 Acc: 0.3684210479259491\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "N = 20000\n",
    "for epoch in range(N):\n",
    "    for step in ['train', 'test']:\n",
    "        \n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        if step == 'train':\n",
    "            loader = train_loader\n",
    "            model.train()\n",
    "        else:\n",
    "            loader = test_loader\n",
    "            model.eval()\n",
    "        for i, (data, target) in enumerate(loader):\n",
    "            inputs = Variable(data)\n",
    "            outputs = Variable(target)\n",
    "            y_pred_val=model(inputs)\n",
    "            acc = calculate_accuracy(y_pred_val, outputs)\n",
    "            accuracies.append(acc)\n",
    "            loss=criterion(y_pred_val, outputs)\n",
    "            losses.append(loss.data)\n",
    "            if step == 'train':\n",
    "                sa_opt.step(model, float(loss.data))\n",
    "        if (epoch % 600 == 0) or (epoch==(N-1)):\n",
    "            print(f'Epoch: {epoch} [{step.capitalize()}]')\n",
    "            print(f'T:{sa_opt.T}, Best: {sa_opt.best_position[0]}, t: {sa_opt.t}')\n",
    "            print(f'Loss: {np.sum(losses)/len(loader)}', f'Acc: {np.sum(accuracies)/len(loader)}')\n",
    "            print('================================================================================')\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaulation of best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_opt.set_parameters(model, sa_opt.best_position[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6607)\n",
      "tensor(0.6053)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizavetabatanina/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "y_pred_val=model(Variable(torch.Tensor(train_x)))\n",
    "print(calculate_accuracy(y_pred_val, Variable(torch.Tensor(train_y))))\n",
    "y_pred_val=model(Variable(torch.Tensor(test_x)))\n",
    "print(calculate_accuracy(y_pred_val, Variable(torch.Tensor(test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
